import torch
import torch.nn as nn
import torch.optim as optim
import math

# Multi-head Attention
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        self.d_k = d_model // num_heads
        self.heads = num_heads
        self.qkv_proj = nn.Linear(d_model, d_model * 3)
        self.out_proj = nn.Linear(d_model, d_model)

    def forward(self, q, k, v, mask=None):
        B, T, C = q.size()
        qkv = self.qkv_proj(q).chunk(3, dim=-1)
        q, k, v = [x.view(B, T, self.heads, self.d_k).transpose(1, 2) for x in qkv]

        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        attn = torch.softmax(scores, dim=-1)
        context = attn @ v
        out = context.transpose(1, 2).contiguous().view(B, T, C)
        return self.out_proj(out)

# Feed Forward
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model),
        )

    def forward(self, x): return self.net(x)

# Positional Encoding
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2) * -math.log(10000.0) / d_model)
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        self.pe = pe.unsqueeze(0)

    def forward(self, x): return x + self.pe[:, :x.size(1)].to(x.device)

# Encoder/Decoder Layer
class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super().__init__()
        self.attn = MultiHeadAttention(d_model, num_heads)
        self.ff = FeedForward(d_model, d_ff)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.drop = nn.Dropout(dropout)

    def forward(self, x, mask):
        x = self.norm1(x + self.drop(self.attn(x, x, x, mask)))
        x = self.norm2(x + self.drop(self.ff(x)))
        return x

class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.cross_attn = MultiHeadAttention(d_model, num_heads)
        self.ff = FeedForward(d_model, d_ff)
        self.norms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(3)])
        self.drop = nn.Dropout(dropout)

    def forward(self, x, enc, src_mask, tgt_mask):
        x = self.norms[0](x + self.drop(self.self_attn(x, x, x, tgt_mask)))
        x = self.norms[1](x + self.drop(self.cross_attn(x, enc, enc, src_mask)))
        return self.norms[2](x + self.drop(self.ff(x)))

# Full Transformer
class Transformer(nn.Module):
    def __init__(self, src_vocab, tgt_vocab, d_model=512, heads=8, layers=6, d_ff=2048, max_len=100, dropout=0.1):
        super().__init__()
        self.src_emb = nn.Embedding(src_vocab, d_model)
        self.tgt_emb = nn.Embedding(tgt_vocab, d_model)
        self.pos_enc = PositionalEncoding(d_model, max_len)
        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, dropout) for _ in range(layers)])
        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, heads, d_ff, dropout) for _ in range(layers)])
        self.fc = nn.Linear(d_model, tgt_vocab)
        self.drop = nn.Dropout(dropout)

    def make_mask(self, src, tgt):
        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)
        size = tgt.size(1)
        nopeak = torch.tril(torch.ones(1, size, size)).bool().to(tgt.device)
        return src_mask, tgt_mask & nopeak

    def forward(self, src, tgt):
        src_mask, tgt_mask = self.make_mask(src, tgt)
        src, tgt = self.drop(self.pos_enc(self.src_emb(src))), self.drop(self.pos_enc(self.tgt_emb(tgt)))
        for l in self.enc_layers: src = l(src, src_mask)
        for l in self.dec_layers: tgt = l(tgt, src, src_mask, tgt_mask)
        return self.fc(tgt)

# Hyperparams & Data
src_vocab = tgt_vocab = 5000
model = Transformer(src_vocab, tgt_vocab)
src = torch.randint(1, src_vocab, (64, 100))
tgt = torch.randint(1, tgt_vocab, (64, 100))

# Training
loss_fn = nn.CrossEntropyLoss(ignore_index=0)
opt = optim.Adam(model.parameters(), lr=1e-4)

for epoch in range(10):
    model.train()
    out = model(src, tgt[:, :-1])
    loss = loss_fn(out.reshape(-1, tgt_vocab), tgt[:, 1:].reshape(-1))
    opt.zero_grad()
    loss.backward()
    opt.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

# Evaluation
model.eval()
with torch.no_grad():
    val_src = torch.randint(1, src_vocab, (64, 100))
    val_tgt = torch.randint(1, tgt_vocab, (64, 100))
    val_out = model(val_src, val_tgt[:, :-1])
    val_loss = loss_fn(val_out.reshape(-1, tgt_vocab), val_tgt[:, 1:].reshape(-1))
    print(f"Validation Loss: {val_loss.item():.4f}")
